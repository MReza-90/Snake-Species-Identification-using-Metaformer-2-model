{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6b6e1b-4bca-4d10-aa19-21d3086351b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch import autocast\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from MetaFormerModel.MetaFG_meta import MetaFG_meta_2\n",
    "\n",
    "from torchmetrics import MeanMetric\n",
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassF1Score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a970475-b972-4f3d-9f9e-8853fba3d849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version\n",
      "3.8.10 (default, Mar 15 2022, 12:22:08) \n",
      "[GCC 9.4.0]\n",
      "Version info.\n",
      "sys.version_info(major=3, minor=8, micro=10, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(\"Python version\")\n",
    "print(sys.version)\n",
    "print(\"Version info.\")\n",
    "print(sys.version_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33170b8e-2905-4976-89b0-b6c3c839ee89",
   "metadata": {},
   "source": [
    "# parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3695ddc1-bef6-419a-8064-669e71396ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Settings #############################\n",
    "#os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "################## Data Paths ##########################\n",
    "MODEL_DIR = \"./metaformer2_CE_4_DA/\"\n",
    "\n",
    "TRAIN_DATA_DIR = \"/raid/SnakeCLEF/snakeclef2023/SnakeCLEF2023/SnakeCLEF2023_train-large_size/\" # train imgs. path on DGX1\n",
    "VAL_DATA_DIR = \"/raid/SnakeCLEF/snakeclef2023/SnakeCLEF2023/SnakeCLEF2023_val-large_size/\" # val imgs. path on DGX1\n",
    "\n",
    "TRAINDATA_CONFIG = \"/raid/SnakeCLEF/snakeclef2023/SnakeCLEF2023/SnakeCLEF2023-TrainMetadata-iNat.csv\"\n",
    "MISSING_FILES = \"/raid/SnakeCLEF/snakeclef2023/SnakeCLEF2023/missing_train_data.csv\"\n",
    "\n",
    "VALIDDATA_CONFIG = \"/raid/SnakeCLEF/snakeclef2023/SnakeCLEF2023/SnakeCLEF2023-ValMetadata.csv\"\n",
    "CCM = \"/raid/SnakeCLEF/snakeclef2023/SnakeCLEF2023/computed/CodeSnakeDistributionTrainObservationLevelBin.csv\"\n",
    "\n",
    "NUM_CLASSES = 1784\n",
    "\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "################## Hyperparameters ########################\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 30\n",
    "IMAGE_SIZE = 384\n",
    "\n",
    "LEARNING_RATE = 1e-05\n",
    "\n",
    "######## Embedding Token Mappings ########################\n",
    "CODE_TOKENS = pickle.load(open(\"meta_code_tokens.p\", \"rb\"))\n",
    "ENDEMIC_TOKENS = pickle.load(open(\"meta_endemic_tokens.p\", \"rb\"))\n",
    "\n",
    "########## Warmup #################\n",
    "WARMUP = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ee89d6-d9ef-4be3-9f4a-bb11c620bb04",
   "metadata": {},
   "source": [
    "### dataset & loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aee5cb85-c7a8-4c21-8f5e-4e9cfb9872dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeTrainDataset(Dataset):\n",
    "    def __init__(self, data, ccm, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform # Image augmentation pipeline\n",
    "        self.code_class_mapping = ccm\n",
    "        self.code_tokens = CODE_TOKENS\n",
    "        self.endemic_tokens = ENDEMIC_TOKENS\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        obj = self.data.iloc[index] # get instance\n",
    "        label = obj.class_id # get label\n",
    "        code = obj.code if obj.code in self.code_tokens.keys() else \"unknown\"\n",
    "        endemic = obj.endemic if obj.endemic in self.endemic_tokens.keys() else \"unknown\" # get endemic metadata\n",
    "                \n",
    "        img = Image.open(obj.image_path).convert(\"RGB\") # load image\n",
    "        ccm = torch.tensor(self.code_class_mapping[code].to_numpy()) # code class mapping\n",
    "        meta = torch.tensor([self.code_tokens[code], self.endemic_tokens[endemic]]).float() # metadata tokens\n",
    "\n",
    "        # img. augmentation\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return (img, label, ccm, meta)\n",
    "\n",
    "\n",
    "# valid data preprocessing pipeline\n",
    "def get_val_preprocessing():\n",
    "    return transforms.Compose([\n",
    "                transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "            ])\n",
    "\n",
    "\n",
    "\n",
    "def get_train_augmentation_preprocessing():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((int(IMAGE_SIZE + IMAGE_SIZE * 0.1), int(IMAGE_SIZE + IMAGE_SIZE * 0.1))),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        \n",
    "        transforms.RandomResizedCrop(IMAGE_SIZE),\n",
    "        transforms.RandomRotation(degrees=30),\n",
    "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "        transforms.GaussianBlur(kernel_size=3),\n",
    "\n",
    "        \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# compute data weights\n",
    "#def compute_weights(dataset):\n",
    "#    CLASS_IDS, CLASS_COUNTS = np.unique(dataset['class_id'], return_counts=True)\n",
    "#    CLASS_WEIGHTS = 1 - (1 / np.sqrt(np.max(CLASS_COUNTS)/CLASS_COUNTS + 0.5))\n",
    "#    return CLASS_WEIGHTS\n",
    "\n",
    "\n",
    "def get_datasets():\n",
    "    # load CSVs\n",
    "    nan_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', '1.#QNAN', '<NA>', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null']\n",
    "    train_data = pd.read_csv(TRAINDATA_CONFIG, na_values=nan_values, keep_default_na=False)\n",
    "    missing_train_data = pd.read_csv(MISSING_FILES, na_values=nan_values, keep_default_na=False)\n",
    "    valid_data = pd.read_csv(VALIDDATA_CONFIG, na_values=nan_values, keep_default_na=False)\n",
    "    \n",
    "    # delete missing files of train data table\n",
    "    train_data = pd.merge(train_data, missing_train_data, how='outer', indicator=True)\n",
    "    train_data = train_data.loc[train_data._merge == 'left_only', [\"observation_id\",\"endemic\",\"binomial_name\",\"code\",\"image_path\",\"class_id\",\"subset\"]]               \n",
    "                                                               \n",
    "    # load transposed version of CCM table\n",
    "    ccm = pd.read_csv(CCM, index_col=0, na_values=nan_values, keep_default_na=False).T\n",
    "\n",
    "    # limit data size\n",
    "    #train_data = train_data.head(1000)\n",
    "    #valid_data = valid_data.head(1000)\n",
    "    print(f'train data shape: {train_data.shape}')\n",
    "\n",
    "    # add image path\n",
    "    train_data[\"image_path\"] = TRAIN_DATA_DIR + train_data['image_path']\n",
    "    valid_data[\"image_path\"] = VAL_DATA_DIR + valid_data['image_path']\n",
    "\n",
    "    # shuffle\n",
    "    train_data = train_data.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "    valid_data = valid_data.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "    # compute train, valid data weights\n",
    "    #TCLASS_WEIGHTS = compute_weights(train_data)\n",
    "    #VCLASS_WEIGHTS = compute_weights(valid_data)\n",
    "\n",
    "    # get train, valid augmentation & preprocessing pipelines\n",
    "    train_aug_preprocessing = get_train_augmentation_preprocessing()\n",
    "    val_preprocessing = get_val_preprocessing()\n",
    "\n",
    "    # create datasets\n",
    "    train_dataset = SnakeTrainDataset(train_data, ccm, transform=train_aug_preprocessing)\n",
    "    valid_dataset = SnakeTrainDataset(valid_data, ccm, transform=val_preprocessing)\n",
    "\n",
    "    return train_dataset, valid_dataset#, TCLASS_WEIGHTS, VCLASS_WEIGHTS\n",
    "\n",
    "\n",
    "def plot_history(logs):\n",
    "    fig, ax = plt.subplots(3, 1, figsize=(8, 12))\n",
    "\n",
    "    ax[0].plot(logs['loss'], label=\"train data\")\n",
    "    ax[0].plot(logs['val_loss'], label=\"valid data\")\n",
    "    ax[0].legend(loc=\"best\")\n",
    "    ax[0].set_ylabel(\"loss\")\n",
    "    ax[0].set_ylim([0, -np.log(1/NUM_CLASSES)])\n",
    "    #ax[0].set_xlabel(\"epochs\")\n",
    "    ax[0].set_title(\"train- vs. valid loss\")\n",
    "\n",
    "    ax[1].plot(logs['acc'], label=\"train data\")\n",
    "    ax[1].plot(logs['val_acc'], label=\"valid data\")\n",
    "    ax[1].legend(loc=\"best\")\n",
    "    ax[1].set_ylabel(\"accuracy\")\n",
    "    ax[1].set_ylim([0, 1.01])\n",
    "    #ax[1].set_xlabel(\"epochs\")\n",
    "    ax[1].set_title(\"train- vs. valid accuracy\")\n",
    "\n",
    "    ax[2].plot(logs['f1'], label=\"train data\")\n",
    "    ax[2].plot(logs['val_f1'], label=\"valid data\")\n",
    "    ax[2].legend(loc=\"best\")\n",
    "    ax[2].set_ylabel(\"f1\")\n",
    "    ax[2].set_ylim([0, 1.01])\n",
    "    ax[2].set_xlabel(\"epochs\")\n",
    "    ax[2].set_title(\"train- vs. valid f1\")\n",
    "\n",
    "    fig.savefig(f'{MODEL_DIR}model_history.svg', dpi=150, format=\"svg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da85eb52-eda3-4d65-8c6c-d1ef02861fcc",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fb7a4f2-30b7-42f2-a5cf-441720de6ba6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohammadreza.bagherifar/.local/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MetaFG_Meta(\n",
       "  (meta_1_head_1): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=512, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (3): ResNormLayer(\n",
       "      (nonlin1): ReLU(inplace=True)\n",
       "      (nonlin2): ReLU(inplace=True)\n",
       "      (norm_fn1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_fn2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (w1): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (w2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (meta_1_head_2): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=1024, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (3): ResNormLayer(\n",
       "      (nonlin1): ReLU(inplace=True)\n",
       "      (nonlin2): ReLU(inplace=True)\n",
       "      (norm_fn1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_fn2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (w1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (w2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (meta_2_head_1): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=512, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (3): ResNormLayer(\n",
       "      (nonlin1): ReLU(inplace=True)\n",
       "      (nonlin2): ReLU(inplace=True)\n",
       "      (norm_fn1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_fn2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (w1): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (w2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (meta_2_head_2): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=1024, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (3): ResNormLayer(\n",
       "      (nonlin1): ReLU(inplace=True)\n",
       "      (nonlin2): ReLU(inplace=True)\n",
       "      (norm_fn1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm_fn2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (w1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (w2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (stage_0): Sequential(\n",
       "    (0): Conv2d(3, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  )\n",
       "  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act1): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (stage_1): ModuleList(\n",
       "    (0): MBConvBlock(\n",
       "      (_expand_conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (_bn0): BatchNorm2d(512, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (_bn1): BatchNorm2d(512, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (_se_expand): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (_project_conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (_bn2): BatchNorm2d(128, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (1): MBConvBlock(\n",
       "      (_expand_conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (_bn0): BatchNorm2d(512, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "      (_bn1): BatchNorm2d(512, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (_se_expand): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (_project_conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (_bn2): BatchNorm2d(128, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "  )\n",
       "  (stage_2): ModuleList(\n",
       "    (0): MBConvBlock(\n",
       "      (_expand_conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (_bn0): BatchNorm2d(512, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
       "      (_bn1): BatchNorm2d(512, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (_se_expand): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (_project_conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (_bn2): BatchNorm2d(256, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (1): MBConvBlock(\n",
       "      (_expand_conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (_bn0): BatchNorm2d(1024, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "      (_bn1): BatchNorm2d(1024, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (_se_expand): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (_project_conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (_bn2): BatchNorm2d(256, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (2): MBConvBlock(\n",
       "      (_expand_conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (_bn0): BatchNorm2d(1024, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "      (_bn1): BatchNorm2d(1024, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (_se_expand): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (_project_conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (_bn2): BatchNorm2d(256, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (3): MBConvBlock(\n",
       "      (_expand_conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (_bn0): BatchNorm2d(1024, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "      (_bn1): BatchNorm2d(1024, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (_se_expand): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (_project_conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (_bn2): BatchNorm2d(256, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (4): MBConvBlock(\n",
       "      (_expand_conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (_bn0): BatchNorm2d(1024, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "      (_bn1): BatchNorm2d(1024, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (_se_expand): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (_project_conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (_bn2): BatchNorm2d(256, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (5): MBConvBlock(\n",
       "      (_expand_conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (_bn0): BatchNorm2d(1024, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "      (_bn1): BatchNorm2d(1024, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (_se_expand): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (_project_conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (_bn2): BatchNorm2d(256, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "  )\n",
       "  (stage_3): ModuleList(\n",
       "    (0): MHSABlock(\n",
       "      (patch_embed): OverlapPatchEmbed(\n",
       "        (proj): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Relative_Attention(\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): MHSABlock(\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Relative_Attention(\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): MHSABlock(\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Relative_Attention(\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): MHSABlock(\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Relative_Attention(\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): MHSABlock(\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Relative_Attention(\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): MHSABlock(\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Relative_Attention(\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): MHSABlock(\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Relative_Attention(\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): MHSABlock(\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Relative_Attention(\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): MHSABlock(\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Relative_Attention(\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): MHSABlock(\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Relative_Attention(\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): MHSABlock(\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Relative_Attention(\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): MHSABlock(\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Relative_Attention(\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (12): MHSABlock(\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Relative_Attention(\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (13): MHSABlock(\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Relative_Attention(\n",
       "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (stage_4): ModuleList(\n",
       "    (0): MHSABlock(\n",
       "      (patch_embed): OverlapPatchEmbed(\n",
       "        (proj): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Relative_Attention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): MHSABlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Relative_Attention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (cl_1_fc): Sequential(\n",
       "    (0): Mlp(\n",
       "      (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (act): GELU(approximate='none')\n",
       "      (fc2): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (aggregate): Conv1d(2, 1, kernel_size=(1,), stride=(1,))\n",
       "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Linear(in_features=1024, out_features=1784, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate the model load checkpoints and move it to the right device\n",
    "model = MetaFG_meta_2(img_size=IMAGE_SIZE, num_classes=NUM_CLASSES, meta_dims=[1,1])\n",
    "\n",
    "# load checkpoints\n",
    "checkpoints = './MetaFormerModel/metafg_2_inat21_384.pth'\n",
    "pretrained_dict = torch.load(checkpoints, map_location='cpu')\n",
    "\n",
    "# get init model params\n",
    "model_dict = model.state_dict()\n",
    "\n",
    "# filter unnecessary keys\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if (k in model_dict) and (model_dict[k].shape == pretrained_dict[k].shape)}\n",
    "model_dict.update(pretrained_dict) \n",
    "\n",
    "# load checkpoints to model\n",
    "model.load_state_dict(model_dict)\n",
    "\n",
    "device = torch.device(f'cuda:0')\n",
    "torch.cuda.set_device(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed1b356d-0734-48e8-a5d9-de0c23f4437d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "MetaFG_Meta                                   [16, 1784]                1,536\n",
      "├─Sequential: 1-1                             [16, 512]                 --\n",
      "│    └─Linear: 2-1                            [16, 512]                 1,024\n",
      "│    └─ReLU: 2-2                              [16, 512]                 --\n",
      "│    └─LayerNorm: 2-3                         [16, 512]                 1,024\n",
      "│    └─ResNormLayer: 2-4                      [16, 512]                 --\n",
      "│    │    └─Linear: 3-1                       [16, 512]                 262,656\n",
      "│    │    └─ReLU: 3-2                         [16, 512]                 --\n",
      "│    │    └─LayerNorm: 3-3                    [16, 512]                 1,024\n",
      "│    │    └─Linear: 3-4                       [16, 512]                 262,656\n",
      "│    │    └─ReLU: 3-5                         [16, 512]                 --\n",
      "│    │    └─LayerNorm: 3-6                    [16, 512]                 1,024\n",
      "├─Sequential: 1-2                             [16, 1024]                --\n",
      "│    └─Linear: 2-5                            [16, 1024]                2,048\n",
      "│    └─ReLU: 2-6                              [16, 1024]                --\n",
      "│    └─LayerNorm: 2-7                         [16, 1024]                2,048\n",
      "│    └─ResNormLayer: 2-8                      [16, 1024]                --\n",
      "│    │    └─Linear: 3-7                       [16, 1024]                1,049,600\n",
      "│    │    └─ReLU: 3-8                         [16, 1024]                --\n",
      "│    │    └─LayerNorm: 3-9                    [16, 1024]                2,048\n",
      "│    │    └─Linear: 3-10                      [16, 1024]                1,049,600\n",
      "│    │    └─ReLU: 3-11                        [16, 1024]                --\n",
      "│    │    └─LayerNorm: 3-12                   [16, 1024]                2,048\n",
      "├─Sequential: 1-3                             [16, 512]                 --\n",
      "│    └─Linear: 2-9                            [16, 512]                 1,024\n",
      "│    └─ReLU: 2-10                             [16, 512]                 --\n",
      "│    └─LayerNorm: 2-11                        [16, 512]                 1,024\n",
      "│    └─ResNormLayer: 2-12                     [16, 512]                 --\n",
      "│    │    └─Linear: 3-13                      [16, 512]                 262,656\n",
      "│    │    └─ReLU: 3-14                        [16, 512]                 --\n",
      "│    │    └─LayerNorm: 3-15                   [16, 512]                 1,024\n",
      "│    │    └─Linear: 3-16                      [16, 512]                 262,656\n",
      "│    │    └─ReLU: 3-17                        [16, 512]                 --\n",
      "│    │    └─LayerNorm: 3-18                   [16, 512]                 1,024\n",
      "├─Sequential: 1-4                             [16, 1024]                --\n",
      "│    └─Linear: 2-13                           [16, 1024]                2,048\n",
      "│    └─ReLU: 2-14                             [16, 1024]                --\n",
      "│    └─LayerNorm: 2-15                        [16, 1024]                2,048\n",
      "│    └─ResNormLayer: 2-16                     [16, 1024]                --\n",
      "│    │    └─Linear: 3-19                      [16, 1024]                1,049,600\n",
      "│    │    └─ReLU: 3-20                        [16, 1024]                --\n",
      "│    │    └─LayerNorm: 3-21                   [16, 1024]                2,048\n",
      "│    │    └─Linear: 3-22                      [16, 1024]                1,049,600\n",
      "│    │    └─ReLU: 3-23                        [16, 1024]                --\n",
      "│    │    └─LayerNorm: 3-24                   [16, 1024]                2,048\n",
      "├─Sequential: 1-5                             [16, 128, 192, 192]       --\n",
      "│    └─Conv2d: 2-17                           [16, 96, 192, 192]        2,592\n",
      "│    └─BatchNorm2d: 2-18                      [16, 96, 192, 192]        192\n",
      "│    └─ReLU: 2-19                             [16, 96, 192, 192]        --\n",
      "│    └─Conv2d: 2-20                           [16, 128, 192, 192]       110,592\n",
      "│    └─BatchNorm2d: 2-21                      [16, 128, 192, 192]       256\n",
      "│    └─ReLU: 2-22                             [16, 128, 192, 192]       --\n",
      "│    └─Conv2d: 2-23                           [16, 128, 192, 192]       147,456\n",
      "├─BatchNorm2d: 1-6                            [16, 128, 192, 192]       256\n",
      "├─ReLU: 1-7                                   [16, 128, 192, 192]       --\n",
      "├─MaxPool2d: 1-8                              [16, 128, 96, 96]         --\n",
      "├─ModuleList: 1-9                             --                        --\n",
      "│    └─MBConvBlock: 2-24                      [16, 128, 96, 96]         --\n",
      "│    │    └─Conv2d: 3-25                      [16, 512, 96, 96]         65,536\n",
      "│    │    └─BatchNorm2d: 3-26                 [16, 512, 96, 96]         1,024\n",
      "│    │    └─MemoryEfficientSwish: 3-27        [16, 512, 96, 96]         --\n",
      "│    │    └─Conv2d: 3-28                      [16, 512, 96, 96]         4,608\n",
      "│    │    └─BatchNorm2d: 3-29                 [16, 512, 96, 96]         1,024\n",
      "│    │    └─MemoryEfficientSwish: 3-30        [16, 512, 96, 96]         --\n",
      "│    │    └─Conv2d: 3-31                      [16, 32, 1, 1]            16,416\n",
      "│    │    └─MemoryEfficientSwish: 3-32        [16, 32, 1, 1]            --\n",
      "│    │    └─Conv2d: 3-33                      [16, 512, 1, 1]           16,896\n",
      "│    │    └─Conv2d: 3-34                      [16, 128, 96, 96]         65,536\n",
      "│    │    └─BatchNorm2d: 3-35                 [16, 128, 96, 96]         256\n",
      "│    └─MBConvBlock: 2-25                      [16, 128, 96, 96]         --\n",
      "│    │    └─Conv2d: 3-36                      [16, 512, 96, 96]         65,536\n",
      "│    │    └─BatchNorm2d: 3-37                 [16, 512, 96, 96]         1,024\n",
      "│    │    └─MemoryEfficientSwish: 3-38        [16, 512, 96, 96]         --\n",
      "│    │    └─Conv2d: 3-39                      [16, 512, 96, 96]         4,608\n",
      "│    │    └─BatchNorm2d: 3-40                 [16, 512, 96, 96]         1,024\n",
      "│    │    └─MemoryEfficientSwish: 3-41        [16, 512, 96, 96]         --\n",
      "│    │    └─Conv2d: 3-42                      [16, 32, 1, 1]            16,416\n",
      "│    │    └─MemoryEfficientSwish: 3-43        [16, 32, 1, 1]            --\n",
      "│    │    └─Conv2d: 3-44                      [16, 512, 1, 1]           16,896\n",
      "│    │    └─Conv2d: 3-45                      [16, 128, 96, 96]         65,536\n",
      "│    │    └─BatchNorm2d: 3-46                 [16, 128, 96, 96]         256\n",
      "├─ModuleList: 1-10                            --                        --\n",
      "│    └─MBConvBlock: 2-26                      [16, 256, 48, 48]         --\n",
      "│    │    └─Conv2d: 3-47                      [16, 512, 96, 96]         65,536\n",
      "│    │    └─BatchNorm2d: 3-48                 [16, 512, 96, 96]         1,024\n",
      "│    │    └─MemoryEfficientSwish: 3-49        [16, 512, 96, 96]         --\n",
      "│    │    └─Conv2d: 3-50                      [16, 512, 48, 48]         4,608\n",
      "│    │    └─BatchNorm2d: 3-51                 [16, 512, 48, 48]         1,024\n",
      "│    │    └─MemoryEfficientSwish: 3-52        [16, 512, 48, 48]         --\n",
      "│    │    └─Conv2d: 3-53                      [16, 32, 1, 1]            16,416\n",
      "│    │    └─MemoryEfficientSwish: 3-54        [16, 32, 1, 1]            --\n",
      "│    │    └─Conv2d: 3-55                      [16, 512, 1, 1]           16,896\n",
      "│    │    └─Conv2d: 3-56                      [16, 256, 48, 48]         131,072\n",
      "│    │    └─BatchNorm2d: 3-57                 [16, 256, 48, 48]         512\n",
      "│    └─MBConvBlock: 2-27                      [16, 256, 48, 48]         --\n",
      "│    │    └─Conv2d: 3-58                      [16, 1024, 48, 48]        262,144\n",
      "│    │    └─BatchNorm2d: 3-59                 [16, 1024, 48, 48]        2,048\n",
      "│    │    └─MemoryEfficientSwish: 3-60        [16, 1024, 48, 48]        --\n",
      "│    │    └─Conv2d: 3-61                      [16, 1024, 48, 48]        9,216\n",
      "│    │    └─BatchNorm2d: 3-62                 [16, 1024, 48, 48]        2,048\n",
      "│    │    └─MemoryEfficientSwish: 3-63        [16, 1024, 48, 48]        --\n",
      "│    │    └─Conv2d: 3-64                      [16, 64, 1, 1]            65,600\n",
      "│    │    └─MemoryEfficientSwish: 3-65        [16, 64, 1, 1]            --\n",
      "│    │    └─Conv2d: 3-66                      [16, 1024, 1, 1]          66,560\n",
      "│    │    └─Conv2d: 3-67                      [16, 256, 48, 48]         262,144\n",
      "│    │    └─BatchNorm2d: 3-68                 [16, 256, 48, 48]         512\n",
      "│    └─MBConvBlock: 2-28                      [16, 256, 48, 48]         --\n",
      "│    │    └─Conv2d: 3-69                      [16, 1024, 48, 48]        262,144\n",
      "│    │    └─BatchNorm2d: 3-70                 [16, 1024, 48, 48]        2,048\n",
      "│    │    └─MemoryEfficientSwish: 3-71        [16, 1024, 48, 48]        --\n",
      "│    │    └─Conv2d: 3-72                      [16, 1024, 48, 48]        9,216\n",
      "│    │    └─BatchNorm2d: 3-73                 [16, 1024, 48, 48]        2,048\n",
      "│    │    └─MemoryEfficientSwish: 3-74        [16, 1024, 48, 48]        --\n",
      "│    │    └─Conv2d: 3-75                      [16, 64, 1, 1]            65,600\n",
      "│    │    └─MemoryEfficientSwish: 3-76        [16, 64, 1, 1]            --\n",
      "│    │    └─Conv2d: 3-77                      [16, 1024, 1, 1]          66,560\n",
      "│    │    └─Conv2d: 3-78                      [16, 256, 48, 48]         262,144\n",
      "│    │    └─BatchNorm2d: 3-79                 [16, 256, 48, 48]         512\n",
      "│    └─MBConvBlock: 2-29                      [16, 256, 48, 48]         --\n",
      "│    │    └─Conv2d: 3-80                      [16, 1024, 48, 48]        262,144\n",
      "│    │    └─BatchNorm2d: 3-81                 [16, 1024, 48, 48]        2,048\n",
      "│    │    └─MemoryEfficientSwish: 3-82        [16, 1024, 48, 48]        --\n",
      "│    │    └─Conv2d: 3-83                      [16, 1024, 48, 48]        9,216\n",
      "│    │    └─BatchNorm2d: 3-84                 [16, 1024, 48, 48]        2,048\n",
      "│    │    └─MemoryEfficientSwish: 3-85        [16, 1024, 48, 48]        --\n",
      "│    │    └─Conv2d: 3-86                      [16, 64, 1, 1]            65,600\n",
      "│    │    └─MemoryEfficientSwish: 3-87        [16, 64, 1, 1]            --\n",
      "│    │    └─Conv2d: 3-88                      [16, 1024, 1, 1]          66,560\n",
      "│    │    └─Conv2d: 3-89                      [16, 256, 48, 48]         262,144\n",
      "│    │    └─BatchNorm2d: 3-90                 [16, 256, 48, 48]         512\n",
      "│    └─MBConvBlock: 2-30                      [16, 256, 48, 48]         --\n",
      "│    │    └─Conv2d: 3-91                      [16, 1024, 48, 48]        262,144\n",
      "│    │    └─BatchNorm2d: 3-92                 [16, 1024, 48, 48]        2,048\n",
      "│    │    └─MemoryEfficientSwish: 3-93        [16, 1024, 48, 48]        --\n",
      "│    │    └─Conv2d: 3-94                      [16, 1024, 48, 48]        9,216\n",
      "│    │    └─BatchNorm2d: 3-95                 [16, 1024, 48, 48]        2,048\n",
      "│    │    └─MemoryEfficientSwish: 3-96        [16, 1024, 48, 48]        --\n",
      "│    │    └─Conv2d: 3-97                      [16, 64, 1, 1]            65,600\n",
      "│    │    └─MemoryEfficientSwish: 3-98        [16, 64, 1, 1]            --\n",
      "│    │    └─Conv2d: 3-99                      [16, 1024, 1, 1]          66,560\n",
      "│    │    └─Conv2d: 3-100                     [16, 256, 48, 48]         262,144\n",
      "│    │    └─BatchNorm2d: 3-101                [16, 256, 48, 48]         512\n",
      "│    └─MBConvBlock: 2-31                      [16, 256, 48, 48]         --\n",
      "│    │    └─Conv2d: 3-102                     [16, 1024, 48, 48]        262,144\n",
      "│    │    └─BatchNorm2d: 3-103                [16, 1024, 48, 48]        2,048\n",
      "│    │    └─MemoryEfficientSwish: 3-104       [16, 1024, 48, 48]        --\n",
      "│    │    └─Conv2d: 3-105                     [16, 1024, 48, 48]        9,216\n",
      "│    │    └─BatchNorm2d: 3-106                [16, 1024, 48, 48]        2,048\n",
      "│    │    └─MemoryEfficientSwish: 3-107       [16, 1024, 48, 48]        --\n",
      "│    │    └─Conv2d: 3-108                     [16, 64, 1, 1]            65,600\n",
      "│    │    └─MemoryEfficientSwish: 3-109       [16, 64, 1, 1]            --\n",
      "│    │    └─Conv2d: 3-110                     [16, 1024, 1, 1]          66,560\n",
      "│    │    └─Conv2d: 3-111                     [16, 256, 48, 48]         262,144\n",
      "│    │    └─BatchNorm2d: 3-112                [16, 256, 48, 48]         512\n",
      "├─ModuleList: 1-11                            --                        --\n",
      "│    └─MHSABlock: 2-32                        [16, 579, 512]            --\n",
      "│    │    └─OverlapPatchEmbed: 3-113          [16, 576, 512]            --\n",
      "│    │    │    └─Conv2d: 4-1                  [16, 512, 24, 24]         1,180,160\n",
      "│    │    │    └─LayerNorm: 4-2               [16, 576, 512]            1,024\n",
      "│    │    └─LayerNorm: 3-114                  [16, 579, 512]            1,024\n",
      "│    │    └─Relative_Attention: 3-115         [16, 579, 512]            17,680\n",
      "│    │    │    └─Linear: 4-3                  [16, 579, 1536]           786,432\n",
      "│    │    │    └─Softmax: 4-4                 [16, 8, 579, 579]         --\n",
      "│    │    │    └─Dropout: 4-5                 [16, 8, 579, 579]         --\n",
      "│    │    │    └─Linear: 4-6                  [16, 579, 512]            262,656\n",
      "│    │    │    └─Dropout: 4-7                 [16, 579, 512]            --\n",
      "│    │    └─Identity: 3-116                   [16, 579, 512]            --\n",
      "│    │    └─LayerNorm: 3-117                  [16, 579, 512]            1,024\n",
      "│    │    └─Mlp: 3-118                        [16, 579, 512]            --\n",
      "│    │    │    └─Linear: 4-8                  [16, 579, 2048]           1,050,624\n",
      "│    │    │    └─GELU: 4-9                    [16, 579, 2048]           --\n",
      "│    │    │    └─Dropout: 4-10                [16, 579, 2048]           --\n",
      "│    │    │    └─Linear: 4-11                 [16, 579, 512]            1,049,088\n",
      "│    │    │    └─Dropout: 4-12                [16, 579, 512]            --\n",
      "│    │    └─Identity: 3-119                   [16, 579, 512]            --\n",
      "│    └─MHSABlock: 2-33                        [16, 579, 512]            --\n",
      "│    │    └─LayerNorm: 3-120                  [16, 579, 512]            1,024\n",
      "│    │    └─Relative_Attention: 3-121         [16, 579, 512]            17,680\n",
      "│    │    │    └─Linear: 4-13                 [16, 579, 1536]           786,432\n",
      "│    │    │    └─Softmax: 4-14                [16, 8, 579, 579]         --\n",
      "│    │    │    └─Dropout: 4-15                [16, 8, 579, 579]         --\n",
      "│    │    │    └─Linear: 4-16                 [16, 579, 512]            262,656\n",
      "│    │    │    └─Dropout: 4-17                [16, 579, 512]            --\n",
      "│    │    └─Identity: 3-122                   [16, 579, 512]            --\n",
      "│    │    └─LayerNorm: 3-123                  [16, 579, 512]            1,024\n",
      "│    │    └─Mlp: 3-124                        [16, 579, 512]            --\n",
      "│    │    │    └─Linear: 4-18                 [16, 579, 2048]           1,050,624\n",
      "│    │    │    └─GELU: 4-19                   [16, 579, 2048]           --\n",
      "│    │    │    └─Dropout: 4-20                [16, 579, 2048]           --\n",
      "│    │    │    └─Linear: 4-21                 [16, 579, 512]            1,049,088\n",
      "│    │    │    └─Dropout: 4-22                [16, 579, 512]            --\n",
      "│    │    └─Identity: 3-125                   [16, 579, 512]            --\n",
      "│    └─MHSABlock: 2-34                        [16, 579, 512]            --\n",
      "│    │    └─LayerNorm: 3-126                  [16, 579, 512]            1,024\n",
      "│    │    └─Relative_Attention: 3-127         [16, 579, 512]            17,680\n",
      "│    │    │    └─Linear: 4-23                 [16, 579, 1536]           786,432\n",
      "│    │    │    └─Softmax: 4-24                [16, 8, 579, 579]         --\n",
      "│    │    │    └─Dropout: 4-25                [16, 8, 579, 579]         --\n",
      "│    │    │    └─Linear: 4-26                 [16, 579, 512]            262,656\n",
      "│    │    │    └─Dropout: 4-27                [16, 579, 512]            --\n",
      "│    │    └─Identity: 3-128                   [16, 579, 512]            --\n",
      "│    │    └─LayerNorm: 3-129                  [16, 579, 512]            1,024\n",
      "│    │    └─Mlp: 3-130                        [16, 579, 512]            --\n",
      "│    │    │    └─Linear: 4-28                 [16, 579, 2048]           1,050,624\n",
      "│    │    │    └─GELU: 4-29                   [16, 579, 2048]           --\n",
      "│    │    │    └─Dropout: 4-30                [16, 579, 2048]           --\n",
      "│    │    │    └─Linear: 4-31                 [16, 579, 512]            1,049,088\n",
      "│    │    │    └─Dropout: 4-32                [16, 579, 512]            --\n",
      "│    │    └─Identity: 3-131                   [16, 579, 512]            --\n",
      "│    └─MHSABlock: 2-35                        [16, 579, 512]            --\n",
      "│    │    └─LayerNorm: 3-132                  [16, 579, 512]            1,024\n",
      "│    │    └─Relative_Attention: 3-133         [16, 579, 512]            17,680\n",
      "│    │    │    └─Linear: 4-33                 [16, 579, 1536]           786,432\n",
      "│    │    │    └─Softmax: 4-34                [16, 8, 579, 579]         --\n",
      "│    │    │    └─Dropout: 4-35                [16, 8, 579, 579]         --\n",
      "│    │    │    └─Linear: 4-36                 [16, 579, 512]            262,656\n",
      "│    │    │    └─Dropout: 4-37                [16, 579, 512]            --\n",
      "│    │    └─Identity: 3-134                   [16, 579, 512]            --\n",
      "│    │    └─LayerNorm: 3-135                  [16, 579, 512]            1,024\n",
      "│    │    └─Mlp: 3-136                        [16, 579, 512]            --\n",
      "│    │    │    └─Linear: 4-38                 [16, 579, 2048]           1,050,624\n",
      "│    │    │    └─GELU: 4-39                   [16, 579, 2048]           --\n",
      "│    │    │    └─Dropout: 4-40                [16, 579, 2048]           --\n",
      "│    │    │    └─Linear: 4-41                 [16, 579, 512]            1,049,088\n",
      "│    │    │    └─Dropout: 4-42                [16, 579, 512]            --\n",
      "│    │    └─Identity: 3-137                   [16, 579, 512]            --\n",
      "│    └─MHSABlock: 2-36                        [16, 579, 512]            --\n",
      "│    │    └─LayerNorm: 3-138                  [16, 579, 512]            1,024\n",
      "│    │    └─Relative_Attention: 3-139         [16, 579, 512]            17,680\n",
      "│    │    │    └─Linear: 4-43                 [16, 579, 1536]           786,432\n",
      "│    │    │    └─Softmax: 4-44                [16, 8, 579, 579]         --\n",
      "│    │    │    └─Dropout: 4-45                [16, 8, 579, 579]         --\n",
      "│    │    │    └─Linear: 4-46                 [16, 579, 512]            262,656\n",
      "│    │    │    └─Dropout: 4-47                [16, 579, 512]            --\n",
      "│    │    └─Identity: 3-140                   [16, 579, 512]            --\n",
      "│    │    └─LayerNorm: 3-141                  [16, 579, 512]            1,024\n",
      "│    │    └─Mlp: 3-142                        [16, 579, 512]            --\n",
      "│    │    │    └─Linear: 4-48                 [16, 579, 2048]           1,050,624\n",
      "│    │    │    └─GELU: 4-49                   [16, 579, 2048]           --\n",
      "│    │    │    └─Dropout: 4-50                [16, 579, 2048]           --\n",
      "│    │    │    └─Linear: 4-51                 [16, 579, 512]            1,049,088\n",
      "│    │    │    └─Dropout: 4-52                [16, 579, 512]            --\n",
      "│    │    └─Identity: 3-143                   [16, 579, 512]            --\n",
      "│    └─MHSABlock: 2-37                        [16, 579, 512]            --\n",
      "│    │    └─LayerNorm: 3-144                  [16, 579, 512]            1,024\n",
      "│    │    └─Relative_Attention: 3-145         [16, 579, 512]            17,680\n",
      "│    │    │    └─Linear: 4-53                 [16, 579, 1536]           786,432\n",
      "│    │    │    └─Softmax: 4-54                [16, 8, 579, 579]         --\n",
      "│    │    │    └─Dropout: 4-55                [16, 8, 579, 579]         --\n",
      "│    │    │    └─Linear: 4-56                 [16, 579, 512]            262,656\n",
      "│    │    │    └─Dropout: 4-57                [16, 579, 512]            --\n",
      "│    │    └─Identity: 3-146                   [16, 579, 512]            --\n",
      "│    │    └─LayerNorm: 3-147                  [16, 579, 512]            1,024\n",
      "│    │    └─Mlp: 3-148                        [16, 579, 512]            --\n",
      "│    │    │    └─Linear: 4-58                 [16, 579, 2048]           1,050,624\n",
      "│    │    │    └─GELU: 4-59                   [16, 579, 2048]           --\n",
      "│    │    │    └─Dropout: 4-60                [16, 579, 2048]           --\n",
      "│    │    │    └─Linear: 4-61                 [16, 579, 512]            1,049,088\n",
      "│    │    │    └─Dropout: 4-62                [16, 579, 512]            --\n",
      "│    │    └─Identity: 3-149                   [16, 579, 512]            --\n",
      "│    └─MHSABlock: 2-38                        [16, 579, 512]            --\n",
      "│    │    └─LayerNorm: 3-150                  [16, 579, 512]            1,024\n",
      "│    │    └─Relative_Attention: 3-151         [16, 579, 512]            17,680\n",
      "│    │    │    └─Linear: 4-63                 [16, 579, 1536]           786,432\n",
      "│    │    │    └─Softmax: 4-64                [16, 8, 579, 579]         --\n",
      "│    │    │    └─Dropout: 4-65                [16, 8, 579, 579]         --\n",
      "│    │    │    └─Linear: 4-66                 [16, 579, 512]            262,656\n",
      "│    │    │    └─Dropout: 4-67                [16, 579, 512]            --\n",
      "│    │    └─Identity: 3-152                   [16, 579, 512]            --\n",
      "│    │    └─LayerNorm: 3-153                  [16, 579, 512]            1,024\n",
      "│    │    └─Mlp: 3-154                        [16, 579, 512]            --\n",
      "│    │    │    └─Linear: 4-68                 [16, 579, 2048]           1,050,624\n",
      "│    │    │    └─GELU: 4-69                   [16, 579, 2048]           --\n",
      "│    │    │    └─Dropout: 4-70                [16, 579, 2048]           --\n",
      "│    │    │    └─Linear: 4-71                 [16, 579, 512]            1,049,088\n",
      "│    │    │    └─Dropout: 4-72                [16, 579, 512]            --\n",
      "│    │    └─Identity: 3-155                   [16, 579, 512]            --\n",
      "│    └─MHSABlock: 2-39                        [16, 579, 512]            --\n",
      "│    │    └─LayerNorm: 3-156                  [16, 579, 512]            1,024\n",
      "│    │    └─Relative_Attention: 3-157         [16, 579, 512]            17,680\n",
      "│    │    │    └─Linear: 4-73                 [16, 579, 1536]           786,432\n",
      "│    │    │    └─Softmax: 4-74                [16, 8, 579, 579]         --\n",
      "│    │    │    └─Dropout: 4-75                [16, 8, 579, 579]         --\n",
      "│    │    │    └─Linear: 4-76                 [16, 579, 512]            262,656\n",
      "│    │    │    └─Dropout: 4-77                [16, 579, 512]            --\n",
      "│    │    └─Identity: 3-158                   [16, 579, 512]            --\n",
      "│    │    └─LayerNorm: 3-159                  [16, 579, 512]            1,024\n",
      "│    │    └─Mlp: 3-160                        [16, 579, 512]            --\n",
      "│    │    │    └─Linear: 4-78                 [16, 579, 2048]           1,050,624\n",
      "│    │    │    └─GELU: 4-79                   [16, 579, 2048]           --\n",
      "│    │    │    └─Dropout: 4-80                [16, 579, 2048]           --\n",
      "│    │    │    └─Linear: 4-81                 [16, 579, 512]            1,049,088\n",
      "│    │    │    └─Dropout: 4-82                [16, 579, 512]            --\n",
      "│    │    └─Identity: 3-161                   [16, 579, 512]            --\n",
      "│    └─MHSABlock: 2-40                        [16, 579, 512]            --\n",
      "│    │    └─LayerNorm: 3-162                  [16, 579, 512]            1,024\n",
      "│    │    └─Relative_Attention: 3-163         [16, 579, 512]            17,680\n",
      "│    │    │    └─Linear: 4-83                 [16, 579, 1536]           786,432\n",
      "│    │    │    └─Softmax: 4-84                [16, 8, 579, 579]         --\n",
      "│    │    │    └─Dropout: 4-85                [16, 8, 579, 579]         --\n",
      "│    │    │    └─Linear: 4-86                 [16, 579, 512]            262,656\n",
      "│    │    │    └─Dropout: 4-87                [16, 579, 512]            --\n",
      "│    │    └─Identity: 3-164                   [16, 579, 512]            --\n",
      "│    │    └─LayerNorm: 3-165                  [16, 579, 512]            1,024\n",
      "│    │    └─Mlp: 3-166                        [16, 579, 512]            --\n",
      "│    │    │    └─Linear: 4-88                 [16, 579, 2048]           1,050,624\n",
      "│    │    │    └─GELU: 4-89                   [16, 579, 2048]           --\n",
      "│    │    │    └─Dropout: 4-90                [16, 579, 2048]           --\n",
      "│    │    │    └─Linear: 4-91                 [16, 579, 512]            1,049,088\n",
      "│    │    │    └─Dropout: 4-92                [16, 579, 512]            --\n",
      "│    │    └─Identity: 3-167                   [16, 579, 512]            --\n",
      "│    └─MHSABlock: 2-41                        [16, 579, 512]            --\n",
      "│    │    └─LayerNorm: 3-168                  [16, 579, 512]            1,024\n",
      "│    │    └─Relative_Attention: 3-169         [16, 579, 512]            17,680\n",
      "│    │    │    └─Linear: 4-93                 [16, 579, 1536]           786,432\n",
      "│    │    │    └─Softmax: 4-94                [16, 8, 579, 579]         --\n",
      "│    │    │    └─Dropout: 4-95                [16, 8, 579, 579]         --\n",
      "│    │    │    └─Linear: 4-96                 [16, 579, 512]            262,656\n",
      "│    │    │    └─Dropout: 4-97                [16, 579, 512]            --\n",
      "│    │    └─Identity: 3-170                   [16, 579, 512]            --\n",
      "│    │    └─LayerNorm: 3-171                  [16, 579, 512]            1,024\n",
      "│    │    └─Mlp: 3-172                        [16, 579, 512]            --\n",
      "│    │    │    └─Linear: 4-98                 [16, 579, 2048]           1,050,624\n",
      "│    │    │    └─GELU: 4-99                   [16, 579, 2048]           --\n",
      "│    │    │    └─Dropout: 4-100               [16, 579, 2048]           --\n",
      "│    │    │    └─Linear: 4-101                [16, 579, 512]            1,049,088\n",
      "│    │    │    └─Dropout: 4-102               [16, 579, 512]            --\n",
      "│    │    └─Identity: 3-173                   [16, 579, 512]            --\n",
      "│    └─MHSABlock: 2-42                        [16, 579, 512]            --\n",
      "│    │    └─LayerNorm: 3-174                  [16, 579, 512]            1,024\n",
      "│    │    └─Relative_Attention: 3-175         [16, 579, 512]            17,680\n",
      "│    │    │    └─Linear: 4-103                [16, 579, 1536]           786,432\n",
      "│    │    │    └─Softmax: 4-104               [16, 8, 579, 579]         --\n",
      "│    │    │    └─Dropout: 4-105               [16, 8, 579, 579]         --\n",
      "│    │    │    └─Linear: 4-106                [16, 579, 512]            262,656\n",
      "│    │    │    └─Dropout: 4-107               [16, 579, 512]            --\n",
      "│    │    └─Identity: 3-176                   [16, 579, 512]            --\n",
      "│    │    └─LayerNorm: 3-177                  [16, 579, 512]            1,024\n",
      "│    │    └─Mlp: 3-178                        [16, 579, 512]            --\n",
      "│    │    │    └─Linear: 4-108                [16, 579, 2048]           1,050,624\n",
      "│    │    │    └─GELU: 4-109                  [16, 579, 2048]           --\n",
      "│    │    │    └─Dropout: 4-110               [16, 579, 2048]           --\n",
      "│    │    │    └─Linear: 4-111                [16, 579, 512]            1,049,088\n",
      "│    │    │    └─Dropout: 4-112               [16, 579, 512]            --\n",
      "│    │    └─Identity: 3-179                   [16, 579, 512]            --\n",
      "│    └─MHSABlock: 2-43                        [16, 579, 512]            --\n",
      "│    │    └─LayerNorm: 3-180                  [16, 579, 512]            1,024\n",
      "│    │    └─Relative_Attention: 3-181         [16, 579, 512]            17,680\n",
      "│    │    │    └─Linear: 4-113                [16, 579, 1536]           786,432\n",
      "│    │    │    └─Softmax: 4-114               [16, 8, 579, 579]         --\n",
      "│    │    │    └─Dropout: 4-115               [16, 8, 579, 579]         --\n",
      "│    │    │    └─Linear: 4-116                [16, 579, 512]            262,656\n",
      "│    │    │    └─Dropout: 4-117               [16, 579, 512]            --\n",
      "│    │    └─Identity: 3-182                   [16, 579, 512]            --\n",
      "│    │    └─LayerNorm: 3-183                  [16, 579, 512]            1,024\n",
      "│    │    └─Mlp: 3-184                        [16, 579, 512]            --\n",
      "│    │    │    └─Linear: 4-118                [16, 579, 2048]           1,050,624\n",
      "│    │    │    └─GELU: 4-119                  [16, 579, 2048]           --\n",
      "│    │    │    └─Dropout: 4-120               [16, 579, 2048]           --\n",
      "│    │    │    └─Linear: 4-121                [16, 579, 512]            1,049,088\n",
      "│    │    │    └─Dropout: 4-122               [16, 579, 512]            --\n",
      "│    │    └─Identity: 3-185                   [16, 579, 512]            --\n",
      "│    └─MHSABlock: 2-44                        [16, 579, 512]            --\n",
      "│    │    └─LayerNorm: 3-186                  [16, 579, 512]            1,024\n",
      "│    │    └─Relative_Attention: 3-187         [16, 579, 512]            17,680\n",
      "│    │    │    └─Linear: 4-123                [16, 579, 1536]           786,432\n",
      "│    │    │    └─Softmax: 4-124               [16, 8, 579, 579]         --\n",
      "│    │    │    └─Dropout: 4-125               [16, 8, 579, 579]         --\n",
      "│    │    │    └─Linear: 4-126                [16, 579, 512]            262,656\n",
      "│    │    │    └─Dropout: 4-127               [16, 579, 512]            --\n",
      "│    │    └─Identity: 3-188                   [16, 579, 512]            --\n",
      "│    │    └─LayerNorm: 3-189                  [16, 579, 512]            1,024\n",
      "│    │    └─Mlp: 3-190                        [16, 579, 512]            --\n",
      "│    │    │    └─Linear: 4-128                [16, 579, 2048]           1,050,624\n",
      "│    │    │    └─GELU: 4-129                  [16, 579, 2048]           --\n",
      "│    │    │    └─Dropout: 4-130               [16, 579, 2048]           --\n",
      "│    │    │    └─Linear: 4-131                [16, 579, 512]            1,049,088\n",
      "│    │    │    └─Dropout: 4-132               [16, 579, 512]            --\n",
      "│    │    └─Identity: 3-191                   [16, 579, 512]            --\n",
      "│    └─MHSABlock: 2-45                        [16, 579, 512]            --\n",
      "│    │    └─LayerNorm: 3-192                  [16, 579, 512]            1,024\n",
      "│    │    └─Relative_Attention: 3-193         [16, 579, 512]            17,680\n",
      "│    │    │    └─Linear: 4-133                [16, 579, 1536]           786,432\n",
      "│    │    │    └─Softmax: 4-134               [16, 8, 579, 579]         --\n",
      "│    │    │    └─Dropout: 4-135               [16, 8, 579, 579]         --\n",
      "│    │    │    └─Linear: 4-136                [16, 579, 512]            262,656\n",
      "│    │    │    └─Dropout: 4-137               [16, 579, 512]            --\n",
      "│    │    └─Identity: 3-194                   [16, 579, 512]            --\n",
      "│    │    └─LayerNorm: 3-195                  [16, 579, 512]            1,024\n",
      "│    │    └─Mlp: 3-196                        [16, 579, 512]            --\n",
      "│    │    │    └─Linear: 4-138                [16, 579, 2048]           1,050,624\n",
      "│    │    │    └─GELU: 4-139                  [16, 579, 2048]           --\n",
      "│    │    │    └─Dropout: 4-140               [16, 579, 2048]           --\n",
      "│    │    │    └─Linear: 4-141                [16, 579, 512]            1,049,088\n",
      "│    │    │    └─Dropout: 4-142               [16, 579, 512]            --\n",
      "│    │    └─Identity: 3-197                   [16, 579, 512]            --\n",
      "├─LayerNorm: 1-12                             [16, 1, 512]              1,024\n",
      "├─Sequential: 1-13                            [16, 1, 1024]             --\n",
      "│    └─Mlp: 2-46                              [16, 1, 1024]             --\n",
      "│    │    └─Linear: 3-198                     [16, 1, 512]              262,656\n",
      "│    │    └─GELU: 3-199                       [16, 1, 512]              --\n",
      "│    │    └─Dropout: 3-200                    [16, 1, 512]              --\n",
      "│    │    └─Linear: 3-201                     [16, 1, 1024]             525,312\n",
      "│    │    └─Dropout: 3-202                    [16, 1, 1024]             --\n",
      "│    └─LayerNorm: 2-47                        [16, 1, 1024]             2,048\n",
      "├─ModuleList: 1-14                            --                        --\n",
      "│    └─MHSABlock: 2-48                        [16, 147, 1024]           --\n",
      "│    │    └─OverlapPatchEmbed: 3-203          [16, 144, 1024]           --\n",
      "│    │    │    └─Conv2d: 4-143                [16, 1024, 12, 12]        4,719,616\n",
      "│    │    │    └─LayerNorm: 4-144             [16, 144, 1024]           2,048\n",
      "│    │    └─LayerNorm: 3-204                  [16, 147, 1024]           2,048\n",
      "│    │    └─Relative_Attention: 3-205         [16, 147, 1024]           4,240\n",
      "│    │    │    └─Linear: 4-145                [16, 147, 3072]           3,145,728\n",
      "│    │    │    └─Softmax: 4-146               [16, 8, 147, 147]         --\n",
      "│    │    │    └─Dropout: 4-147               [16, 8, 147, 147]         --\n",
      "│    │    │    └─Linear: 4-148                [16, 147, 1024]           1,049,600\n",
      "│    │    │    └─Dropout: 4-149               [16, 147, 1024]           --\n",
      "│    │    └─Identity: 3-206                   [16, 147, 1024]           --\n",
      "│    │    └─LayerNorm: 3-207                  [16, 147, 1024]           2,048\n",
      "│    │    └─Mlp: 3-208                        [16, 147, 1024]           --\n",
      "│    │    │    └─Linear: 4-150                [16, 147, 4096]           4,198,400\n",
      "│    │    │    └─GELU: 4-151                  [16, 147, 4096]           --\n",
      "│    │    │    └─Dropout: 4-152               [16, 147, 4096]           --\n",
      "│    │    │    └─Linear: 4-153                [16, 147, 1024]           4,195,328\n",
      "│    │    │    └─Dropout: 4-154               [16, 147, 1024]           --\n",
      "│    │    └─Identity: 3-209                   [16, 147, 1024]           --\n",
      "│    └─MHSABlock: 2-49                        [16, 147, 1024]           --\n",
      "│    │    └─LayerNorm: 3-210                  [16, 147, 1024]           2,048\n",
      "│    │    └─Relative_Attention: 3-211         [16, 147, 1024]           4,240\n",
      "│    │    │    └─Linear: 4-155                [16, 147, 3072]           3,145,728\n",
      "│    │    │    └─Softmax: 4-156               [16, 8, 147, 147]         --\n",
      "│    │    │    └─Dropout: 4-157               [16, 8, 147, 147]         --\n",
      "│    │    │    └─Linear: 4-158                [16, 147, 1024]           1,049,600\n",
      "│    │    │    └─Dropout: 4-159               [16, 147, 1024]           --\n",
      "│    │    └─Identity: 3-212                   [16, 147, 1024]           --\n",
      "│    │    └─LayerNorm: 3-213                  [16, 147, 1024]           2,048\n",
      "│    │    └─Mlp: 3-214                        [16, 147, 1024]           --\n",
      "│    │    │    └─Linear: 4-160                [16, 147, 4096]           4,198,400\n",
      "│    │    │    └─GELU: 4-161                  [16, 147, 4096]           --\n",
      "│    │    │    └─Dropout: 4-162               [16, 147, 4096]           --\n",
      "│    │    │    └─Linear: 4-163                [16, 147, 1024]           4,195,328\n",
      "│    │    │    └─Dropout: 4-164               [16, 147, 1024]           --\n",
      "│    │    └─Identity: 3-215                   [16, 147, 1024]           --\n",
      "├─LayerNorm: 1-15                             [16, 1, 1024]             2,048\n",
      "├─Conv1d: 1-16                                [16, 1, 1024]             3\n",
      "├─LayerNorm: 1-17                             [16, 1024]                2,048\n",
      "├─Linear: 1-18                                [16, 1784]                1,828,600\n",
      "===============================================================================================\n",
      "Total params: 87,548,283\n",
      "Trainable params: 87,548,283\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 329.74\n",
      "===============================================================================================\n",
      "Input size (MB): 28.31\n",
      "Forward/backward pass size (MB): 23598.42\n",
      "Params size (MB): 349.16\n",
      "Estimated Total Size (MB): 23975.89\n",
      "===============================================================================================\n",
      "MetaFG_Meta(\n",
      "  (meta_1_head_1): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=512, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): ResNormLayer(\n",
      "      (nonlin1): ReLU(inplace=True)\n",
      "      (nonlin2): ReLU(inplace=True)\n",
      "      (norm_fn1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm_fn2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (w1): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (w2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (meta_1_head_2): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=1024, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): ResNormLayer(\n",
      "      (nonlin1): ReLU(inplace=True)\n",
      "      (nonlin2): ReLU(inplace=True)\n",
      "      (norm_fn1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm_fn2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (w1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (w2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (meta_2_head_1): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=512, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): ResNormLayer(\n",
      "      (nonlin1): ReLU(inplace=True)\n",
      "      (nonlin2): ReLU(inplace=True)\n",
      "      (norm_fn1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm_fn2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (w1): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (w2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (meta_2_head_2): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=1024, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): ResNormLayer(\n",
      "      (nonlin1): ReLU(inplace=True)\n",
      "      (nonlin2): ReLU(inplace=True)\n",
      "      (norm_fn1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm_fn2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (w1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (w2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (stage_0): Sequential(\n",
      "    (0): Conv2d(3, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (act1): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (stage_1): ModuleList(\n",
      "    (0): MBConvBlock(\n",
      "      (_expand_conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (_bn0): BatchNorm2d(512, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (_bn1): BatchNorm2d(512, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_se_expand): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_project_conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (_bn2): BatchNorm2d(128, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (1): MBConvBlock(\n",
      "      (_expand_conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (_bn0): BatchNorm2d(512, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "      (_bn1): BatchNorm2d(512, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_se_expand): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_project_conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (_bn2): BatchNorm2d(128, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "  )\n",
      "  (stage_2): ModuleList(\n",
      "    (0): MBConvBlock(\n",
      "      (_expand_conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (_bn0): BatchNorm2d(512, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
      "      (_bn1): BatchNorm2d(512, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_se_expand): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_project_conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (_bn2): BatchNorm2d(256, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (1): MBConvBlock(\n",
      "      (_expand_conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (_bn0): BatchNorm2d(1024, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "      (_bn1): BatchNorm2d(1024, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_se_expand): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_project_conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (_bn2): BatchNorm2d(256, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (2): MBConvBlock(\n",
      "      (_expand_conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (_bn0): BatchNorm2d(1024, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "      (_bn1): BatchNorm2d(1024, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_se_expand): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_project_conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (_bn2): BatchNorm2d(256, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (3): MBConvBlock(\n",
      "      (_expand_conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (_bn0): BatchNorm2d(1024, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "      (_bn1): BatchNorm2d(1024, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_se_expand): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_project_conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (_bn2): BatchNorm2d(256, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (4): MBConvBlock(\n",
      "      (_expand_conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (_bn0): BatchNorm2d(1024, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "      (_bn1): BatchNorm2d(1024, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_se_expand): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_project_conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (_bn2): BatchNorm2d(256, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "    (5): MBConvBlock(\n",
      "      (_expand_conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (_bn0): BatchNorm2d(1024, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_depthwise_conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
      "      (_bn1): BatchNorm2d(1024, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_se_reduce): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_se_expand): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (_project_conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (_bn2): BatchNorm2d(256, eps=0.01, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (_swish): MemoryEfficientSwish()\n",
      "    )\n",
      "  )\n",
      "  (stage_3): ModuleList(\n",
      "    (0): MHSABlock(\n",
      "      (patch_embed): OverlapPatchEmbed(\n",
      "        (proj): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Relative_Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): MHSABlock(\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Relative_Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): MHSABlock(\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Relative_Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): MHSABlock(\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Relative_Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): MHSABlock(\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Relative_Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): MHSABlock(\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Relative_Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): MHSABlock(\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Relative_Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): MHSABlock(\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Relative_Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): MHSABlock(\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Relative_Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): MHSABlock(\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Relative_Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): MHSABlock(\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Relative_Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): MHSABlock(\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Relative_Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (12): MHSABlock(\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Relative_Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (13): MHSABlock(\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Relative_Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (stage_4): ModuleList(\n",
      "    (0): MHSABlock(\n",
      "      (patch_embed): OverlapPatchEmbed(\n",
      "        (proj): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Relative_Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): MHSABlock(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Relative_Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (cl_1_fc): Sequential(\n",
      "    (0): Mlp(\n",
      "      (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (act): GELU(approximate='none')\n",
      "      (fc2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      (drop): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (aggregate): Conv1d(2, 1, kernel_size=(1,), stride=(1,))\n",
      "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (head): Linear(in_features=1024, out_features=1784, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(BATCH_SIZE, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "meta = torch.randn(BATCH_SIZE,2)\n",
    "print(summary(model, input_data={'x': x, 'meta': meta}, depth=5, device=device))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43c97a5-2fa7-49ca-8da2-9a3b1e25fa00",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d99a0821-72fa-4f88-8e60-a06561ee0935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup(model):\n",
    "    for i, (param_name, param) in enumerate(model.named_parameters()):\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    for i, (param_name, param) in enumerate(model.meta_1_head_1.named_parameters()):\n",
    "        param.requires_grad = True\n",
    "        \n",
    "    for i, (param_name, param) in enumerate(model.meta_1_head_2.named_parameters()):\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    for i, (param_name, param) in enumerate(model.meta_2_head_1.named_parameters()):\n",
    "        param.requires_grad = True\n",
    "        \n",
    "    for i, (param_name, param) in enumerate(model.meta_2_head_2.named_parameters()):\n",
    "        param.requires_grad = True\n",
    "        \n",
    "    for i, (param_name, param) in enumerate(model.head.named_parameters()):\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    \n",
    "def warmup_end(model):\n",
    "    for i, (param_name, param) in enumerate(model.named_parameters()):\n",
    "        param.requires_grad = True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c7f72b4-4abe-4631-b1c8-d62ee691c94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (154216, 7)\n"
     ]
    }
   ],
   "source": [
    "train_dataset, valid_dataset = get_datasets()\n",
    "\n",
    "# prepare the datasets\n",
    "train_loader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=BATCH_SIZE, num_workers=8, drop_last=False, pin_memory=True)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, shuffle=False, batch_size=BATCH_SIZE, num_workers=8, drop_last=False, pin_memory=True)\n",
    "\n",
    "\n",
    "# Optimizer & Schedules & early stopping\n",
    "#optimizer = torch.optim.Adam([{'params': model.conv_backbone.parameters(), 'lr': LEARNING_RATE},\n",
    "#                            {'params': model.embedding_loss.parameters(), 'lr': LEARNING_RATE*10},\n",
    "#                            {'params': model.endemic_embedding.parameters(), 'lr': LEARNING_RATE * 10},\n",
    "#                            {'params': model.code_embedding.parameters(), 'lr': LEARNING_RATE * 10},\n",
    "#                            {'params': model.embedding_net.parameters(), 'lr': LEARNING_RATE * 10}\n",
    "#                            ])\n",
    "\n",
    "# Optimizer & Schedules & early stopping\n",
    "#optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "warmup(model)\n",
    "optimizer = torch.optim.AdamW([{'params': filter(lambda p: p.requires_grad, model.parameters()), 'lr': LEARNING_RATE*10}])\n",
    "scaler = GradScaler()\n",
    "\n",
    "# loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# running metrics during training\n",
    "loss_metric = MeanMetric().to(device)\n",
    "\n",
    "acc_metric = MulticlassAccuracy(num_classes=NUM_CLASSES, average='macro').to(device)\n",
    "f1_metric = MulticlassF1Score(num_classes=NUM_CLASSES, average='macro').to(device)\n",
    "f1country_metric = MulticlassF1Score(num_classes=NUM_CLASSES, average='macro').to(device)\n",
    "\n",
    "# start time of trainig\n",
    "start_training = time.perf_counter()\n",
    "# create log dict\n",
    "logs = {'loss': [], 'acc': [], 'f1': [], 'f1country': [], 'val_loss': [], 'val_acc': [], 'val_f1': [], 'val_f1country': [], 'learning_rate': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73250355-f9b6-4abb-a647-8d5d8ceefa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # start a new wandb run to track this script\n",
    "# wandb.init(\n",
    "#     entity=\"snakeclef2023_fhdo\", # our team at wandb\n",
    "\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"meta_former\", # -> define sub-projects here, e.g. experiments with MetaFormer or CNNs...\n",
    "    \n",
    "#     # define a name for this run\n",
    "#     name=\"MetaFormer2_init_run\",\n",
    "\n",
    "#     # track all the used hyperparameters here, config is just a dict object so any key:value pairs are possible\n",
    "#     config={\n",
    "#         \"learning_rate\": LEARNING_RATE,\n",
    "#         \"architecture\": \"MetaFormer2\",\n",
    "#         \"pretrained\": \"iNat21\",\n",
    "#         \"dataset\": \"snakeclef2023, additional training data not used...\",\n",
    "#         \"epochs\": NUM_EPOCHS,\n",
    "#         \"img_size\": IMAGE_SIZE,\n",
    "#         \"batch_size\": BATCH_SIZE,\n",
    "#         \"code_tokens\": CODE_TOKENS,\n",
    "#         \"endemic_tokens\": ENDEMIC_TOKENS,\n",
    "#         \"embedding_warmup_epochs\": WARMUP\n",
    "#         # ... any other hyperparameter that is necessary to reproduce the result\n",
    "#     },\n",
    "    \n",
    "#     #save_code=True, # save the script file as backup\n",
    "    \n",
    "#     dir=MODEL_DIR # locally folder where wandb log files are saved \n",
    "# )\n",
    "  \n",
    "# # (optional) wandb can also track the model parameters\n",
    "# # wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "999e8402-fc3b-49d3-a8ac-7b8162197e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "--> Warm Up 1/3\n",
      "loss: 3.68721, acc: 0.15132, f1: 0.16464, f1country: 0.27423 - val_loss: 2.23551, val_acc: 0.30490, val_f1: 0.28204, val_f1country: 0.37183 - lr: 0.0001 - epoch runtime: 1837.043 sec.\n",
      "Epoch 2/30\n",
      "--> Warm Up 2/3\n",
      "loss: 2.24467, acc: 0.34171, f1: 0.36726, f1country: 0.48193 - val_loss: 1.79860, val_acc: 0.39284, val_f1: 0.36949, val_f1country: 0.44726 - lr: 0.0001 - epoch runtime: 1833.855 sec.\n",
      "Epoch 3/30\n",
      "--> Warm Up 3/3\n",
      "loss: 1.91914, acc: 0.43333, f1: 0.46518, f1country: 0.56347 - val_loss: 1.67436, val_acc: 0.42206, val_f1: 0.39862, val_f1country: 0.47025 - lr: 0.0001 - epoch runtime: 1834.709 sec.\n",
      "Epoch 4/30\n",
      "loss: 1.28502, acc: 0.59641, f1: 0.63146, f1country: 0.69853 - val_loss: 1.28911, val_acc: 0.49782, val_f1: 0.47709, val_f1country: 0.52494 - lr: 1e-05 - epoch runtime: 3741.572 sec.\n",
      "Epoch 5/30\n",
      "loss: 0.96625, acc: 0.67740, f1: 0.71329, f1country: 0.75593 - val_loss: 1.16129, val_acc: 0.52263, val_f1: 0.50292, val_f1country: 0.53940 - lr: 1e-05 - epoch runtime: 3733.654 sec.\n",
      "Epoch 6/30\n",
      "loss: 0.80961, acc: 0.72617, f1: 0.75962, f1country: 0.79097 - val_loss: 1.07737, val_acc: 0.55283, val_f1: 0.53022, val_f1country: 0.56189 - lr: 1e-05 - epoch runtime: 3718.717 sec.\n",
      "Epoch 7/30\n",
      "loss: 0.70323, acc: 0.76549, f1: 0.79698, f1country: 0.82058 - val_loss: 1.02589, val_acc: 0.56032, val_f1: 0.53883, val_f1country: 0.56251 - lr: 1e-05 - epoch runtime: 3681.286 sec.\n",
      "Epoch 8/30\n",
      "loss: 0.62188, acc: 0.79366, f1: 0.82360, f1country: 0.84093 - val_loss: 1.00923, val_acc: 0.56806, val_f1: 0.54627, val_f1country: 0.57597 - lr: 1e-05 - epoch runtime: 3699.410 sec.\n",
      "Epoch 9/30\n",
      "loss: 0.55506, acc: 0.81738, f1: 0.84342, f1country: 0.85769 - val_loss: 0.95948, val_acc: 0.57201, val_f1: 0.55250, val_f1country: 0.57345 - lr: 1e-05 - epoch runtime: 3706.928 sec.\n",
      "Epoch 10/30\n",
      "loss: 0.50019, acc: 0.83566, f1: 0.86124, f1country: 0.87392 - val_loss: 0.95110, val_acc: 0.58088, val_f1: 0.56054, val_f1country: 0.58595 - lr: 1e-05 - epoch runtime: 3720.168 sec.\n",
      "Epoch 11/30\n",
      "loss: 0.45675, acc: 0.85478, f1: 0.87883, f1country: 0.88875 - val_loss: 0.94641, val_acc: 0.58778, val_f1: 0.56730, val_f1country: 0.58798 - lr: 1e-05 - epoch runtime: 3687.424 sec.\n",
      "Epoch 12/30\n",
      "loss: 0.42348, acc: 0.86737, f1: 0.88916, f1country: 0.89726 - val_loss: 0.92977, val_acc: 0.58918, val_f1: 0.56594, val_f1country: 0.58236 - lr: 1e-05 - epoch runtime: 3696.609 sec.\n",
      "Epoch 13/30\n",
      "loss: 0.39071, acc: 0.88408, f1: 0.90317, f1country: 0.91015 - val_loss: 0.95241, val_acc: 0.59023, val_f1: 0.57163, val_f1country: 0.59578 - lr: 1e-05 - epoch runtime: 3718.489 sec.\n",
      "Epoch 14/30\n",
      "loss: 0.36135, acc: 0.89125, f1: 0.91044, f1country: 0.91657 - val_loss: 0.94893, val_acc: 0.59686, val_f1: 0.57927, val_f1country: 0.60174 - lr: 1e-05 - epoch runtime: 3699.957 sec.\n",
      "Epoch 15/30\n",
      "loss: 0.33688, acc: 0.89973, f1: 0.91756, f1country: 0.92299 - val_loss: 0.99717, val_acc: 0.59038, val_f1: 0.57388, val_f1country: 0.59247 - lr: 1e-05 - epoch runtime: 3691.794 sec.\n",
      "Epoch 16/30\n",
      "loss: 0.32140, acc: 0.90914, f1: 0.92489, f1country: 0.93002 - val_loss: 0.96047, val_acc: 0.58429, val_f1: 0.56491, val_f1country: 0.58835 - lr: 1e-05 - epoch runtime: 3722.377 sec.\n",
      "Epoch 17/30\n",
      "loss: 0.30297, acc: 0.91466, f1: 0.93000, f1country: 0.93372 - val_loss: 0.96870, val_acc: 0.58847, val_f1: 0.56991, val_f1country: 0.58842 - lr: 1e-05 - epoch runtime: 3673.348 sec.\n",
      "Epoch 18/30\n",
      "loss: 0.28848, acc: 0.92147, f1: 0.93522, f1country: 0.93920 - val_loss: 0.96573, val_acc: 0.59808, val_f1: 0.57952, val_f1country: 0.59708 - lr: 1e-05 - epoch runtime: 3739.098 sec.\n",
      "Epoch 19/30\n",
      "loss: 0.27694, acc: 0.92328, f1: 0.93700, f1country: 0.94086 - val_loss: 0.99207, val_acc: 0.59817, val_f1: 0.57888, val_f1country: 0.59603 - lr: 1e-05 - epoch runtime: 3693.085 sec.\n",
      "Epoch 20/30\n",
      "loss: 0.26228, acc: 0.92691, f1: 0.94007, f1country: 0.94346 - val_loss: 0.99894, val_acc: 0.59100, val_f1: 0.57193, val_f1country: 0.59149 - lr: 1e-05 - epoch runtime: 3722.695 sec.\n",
      "Epoch 21/30\n",
      "loss: 0.25241, acc: 0.93171, f1: 0.94371, f1country: 0.94692 - val_loss: 1.00323, val_acc: 0.59700, val_f1: 0.58022, val_f1country: 0.60126 - lr: 1e-05 - epoch runtime: 3718.976 sec.\n",
      "Epoch 22/30\n",
      "loss: 0.24491, acc: 0.93507, f1: 0.94631, f1country: 0.94928 - val_loss: 0.99986, val_acc: 0.59980, val_f1: 0.58271, val_f1country: 0.59982 - lr: 1e-05 - epoch runtime: 3697.024 sec.\n",
      "Epoch 23/30\n",
      "loss: 0.23696, acc: 0.93809, f1: 0.94883, f1country: 0.95171 - val_loss: 1.00024, val_acc: 0.59520, val_f1: 0.57956, val_f1country: 0.59726 - lr: 1e-05 - epoch runtime: 3709.067 sec.\n",
      "Epoch 24/30\n",
      "loss: 0.23055, acc: 0.93916, f1: 0.94969, f1country: 0.95249 - val_loss: 1.04229, val_acc: 0.58538, val_f1: 0.57072, val_f1country: 0.59634 - lr: 1e-05 - epoch runtime: 3719.588 sec.\n",
      "Epoch 25/30\n",
      "loss: 0.22270, acc: 0.94325, f1: 0.95275, f1country: 0.95571 - val_loss: 1.01743, val_acc: 0.59916, val_f1: 0.58166, val_f1country: 0.59691 - lr: 1e-05 - epoch runtime: 3729.245 sec.\n",
      "Epoch 26/30\n",
      "loss: 0.21802, acc: 0.94276, f1: 0.95246, f1country: 0.95514 - val_loss: 1.02847, val_acc: 0.59578, val_f1: 0.57480, val_f1country: 0.59497 - lr: 1e-05 - epoch runtime: 3690.754 sec.\n",
      "Epoch 27/30\n",
      "loss: 0.21454, acc: 0.94693, f1: 0.95556, f1country: 0.95756 - val_loss: 1.04027, val_acc: 0.59152, val_f1: 0.57362, val_f1country: 0.59476 - lr: 1e-05 - epoch runtime: 3712.900 sec.\n",
      "Epoch 28/30\n",
      "loss: 0.21116, acc: 0.94513, f1: 0.95442, f1country: 0.95702 - val_loss: 1.07199, val_acc: 0.59178, val_f1: 0.57367, val_f1country: 0.58971 - lr: 1e-05 - epoch runtime: 3698.569 sec.\n",
      "Epoch 29/30\n",
      "loss: 0.20477, acc: 0.94716, f1: 0.95610, f1country: 0.95865 - val_loss: 1.06271, val_acc: 0.59443, val_f1: 0.57933, val_f1country: 0.59653 - lr: 1e-05 - epoch runtime: 3722.217 sec.\n",
      "Epoch 30/30\n",
      "loss: 0.19723, acc: 0.94721, f1: 0.95636, f1country: 0.95877 - val_loss: 1.06254, val_acc: 0.60021, val_f1: 0.58234, val_f1country: 0.59626 - lr: 1e-05 - epoch runtime: 3723.722 sec.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x864 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training succeeded in 105675.724s\n"
     ]
    }
   ],
   "source": [
    "#iterate over epochs\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # start time of epoch\n",
    "    epoch_start = time.perf_counter()\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
    "    \n",
    "    ######################## toggle warmup ########################################\n",
    "    if (epoch) == WARMUP:\n",
    "        warmup_end(model)\n",
    "        optimizer = torch.optim.AdamW([{'params': filter(lambda p: p.requires_grad, model.parameters()), 'lr': LEARNING_RATE}])\n",
    "    elif (epoch) < WARMUP:\n",
    "        print(f'--> Warm Up {epoch+1}/{WARMUP}')\n",
    "\n",
    "    ############################## train phase ####################################\n",
    "    model.train()\n",
    "\n",
    "    # iterate over training batches\n",
    "    for (inputs, labels, ccm, meta) in train_loader:\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        ccm = ccm.to(device, non_blocking=True)\n",
    "        meta = meta.to(device, non_blocking=True)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # forward with mixed precision\n",
    "        with autocast(device_type='cuda', dtype=torch.float16):\n",
    "            outputs = model(x=inputs, meta=meta)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # backward + optimize with gradient clipping to 1 with mixed precision\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Compute metrics\n",
    "        preds = outputs.softmax(dim=-1)\n",
    "        loss_metric.update(loss)\n",
    "        acc_metric.update(preds, labels)\n",
    "        f1_metric.update(preds, labels)\n",
    "        f1country_metric.update(preds * ccm, labels)\n",
    "\n",
    "    # compute, sync & reset metrics for validation\n",
    "    epoch_loss = loss_metric.compute()\n",
    "    epoch_acc = acc_metric.compute()\n",
    "    epoch_f1 = f1_metric.compute()\n",
    "    epoch_f1country = f1country_metric.compute()\n",
    "    loss_metric.reset()\n",
    "    acc_metric.reset()\n",
    "    f1_metric.reset()\n",
    "    f1country_metric.reset()\n",
    "\n",
    "    # Append metric results to logs\n",
    "    logs['loss'].append(epoch_loss.detach().cpu().item())\n",
    "    logs['acc'].append(epoch_acc.detach().cpu().item())\n",
    "    logs['f1'].append(epoch_f1.detach().cpu().item())\n",
    "    logs['f1country'].append(epoch_f1country.detach().cpu().item())\n",
    "    print(f\"loss: {logs['loss'][epoch]:.5f}, acc: {logs['acc'][epoch]:.5f}, f1: {logs['f1'][epoch]:.5f}, f1country: {logs['f1country'][epoch]:.5f}\", end=' - ')\n",
    "\n",
    "\n",
    "    ############################## valid phase ####################################\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        # iterate over validation batches\n",
    "        for (inputs, labels, ccm, meta) in valid_loader:\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            ccm = ccm.to(device, non_blocking=True)\n",
    "            meta = meta.to(device, non_blocking=True)\n",
    "\n",
    "            # forward with mixed precision\n",
    "            with autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = model(x=inputs, meta=meta)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "\n",
    "            # Compute metrics\n",
    "            val_preds = outputs.softmax(dim=-1)\n",
    "            loss_metric.update(loss)\n",
    "            acc_metric.update(val_preds, labels)\n",
    "            f1_metric.update(val_preds, labels)\n",
    "            f1country_metric.update(val_preds * ccm, labels)\n",
    "\n",
    "        # compute, sync & reset metrics for validation\n",
    "        epoch_val_loss = loss_metric.compute()\n",
    "        epoch_val_acc = acc_metric.compute()\n",
    "        epoch_val_f1 = f1_metric.compute()\n",
    "        epoch_val_f1country = f1country_metric.compute()\n",
    "        loss_metric.reset()\n",
    "        acc_metric.reset()\n",
    "        f1_metric.reset()\n",
    "        f1country_metric.reset()\n",
    "\n",
    "        # get epoch learning rate\n",
    "        epoch_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    # Append lr, metric results to logs\n",
    "    logs['learning_rate'].append(epoch_lr if isinstance(epoch_lr, float) else epoch_lr.detach().cpu().item())\n",
    "    logs['val_loss'].append(epoch_val_loss.detach().cpu().item())\n",
    "    logs['val_acc'].append(epoch_val_acc.detach().cpu().item())\n",
    "    logs['val_f1'].append(epoch_val_f1.detach().cpu().item())\n",
    "    logs['val_f1country'].append(epoch_val_f1country.detach().cpu().item())\n",
    "    print(f\"val_loss: {logs['val_loss'][epoch]:.5f}, val_acc: {logs['val_acc'][epoch]:.5f}, val_f1: {logs['val_f1'][epoch]:.5f}, val_f1country: {logs['val_f1country'][epoch]:.5f} - lr: {epoch_lr}\", end=' - ')\n",
    "\n",
    "    # save logs as csv\n",
    "    logs_df = pd.DataFrame(logs)\n",
    "    logs_df.to_csv(f'{MODEL_DIR}train_history.csv', index_label='epoch', sep=',', encoding='utf-8')\n",
    "    \n",
    "    # save logs to wandb for this epoch\n",
    "    # wandb.log(\n",
    "    #     {k:v[epoch] for k,v in logs.items()}, # e.g. log each metric value for the current epoch in our defined logs dict\n",
    "    #     step=epoch # epoch index for wandb\n",
    "    # )\n",
    "\n",
    "    #save trained model for each epoch\n",
    "    torch.save(model.state_dict(), f'{MODEL_DIR}model_epoch{epoch}.pth')\n",
    "    torch.save(optimizer.state_dict(), f'{MODEL_DIR}optimizer_epoch{epoch}.pth')\n",
    "\n",
    "    # end time of epoch\n",
    "    epoch_end = time.perf_counter()\n",
    "    print(f\"epoch runtime: {epoch_end-epoch_start:5.3f} sec.\")\n",
    "\n",
    "plot_history(logs)\n",
    "# end time of trainig\n",
    "end_training = time.perf_counter()\n",
    "print(f'Training succeeded in {(end_training - start_training):5.3f}s')\n",
    "\n",
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504c127c-7870-4ea4-9bed-7aaf943793a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
